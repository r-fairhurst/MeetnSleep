import unittest
import json
import os
import sys
from tqdm import tqdm
import Levenshtein # pip install python-Levenshtein

# Add the parent directory to sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from prototype.transcription import transcribe_audio

# Tests generated by ChatGPT
# INSTRUCTIONS:
#   Navigate to the 'backend' directory
#   Run this command in the terminal: 'python -m unittest discover prototype/tests'

class TestTranscription(unittest.TestCase):
    def setUp(self):
        """Load expected outputs before running tests."""
        expected_outputs_path = os.path.join(os.path.dirname(__file__), "expected_outputs.json")
        with open(expected_outputs_path, "r") as f:
            self.expected_outputs = json.load(f)
        
        self.test_folder = os.path.join(os.path.dirname(__file__), "test_inputs")

    def calculate_similarity(self, expected, actual):
        """Calculate transcription accuracy as a percentage using Levenshtein distance."""
        distance = Levenshtein.distance(expected, actual)
        max_len = max(len(expected), len(actual))
        similarity = (1 - distance / max_len) * 100 if max_len > 0 else 100
        return similarity

    def test_transcription_accuracy(self):
        """Test each audio file and print similarity score."""
        
        total_score = 0
        num_tests = 0

        # Wrap the loop with tqdm to show a progress bar
        for filename, expected_text in tqdm(self.expected_outputs.items(), 
                                            desc="Testing", 
                                            ncols=75, 
                                            ascii=False,
                                            position=num_tests,
                                            leave=True):
            audio_path = os.path.join(self.test_folder, filename)
            actual_transcription = transcribe_audio(audio_path)

            if actual_transcription is None:
                print(f"ERROR: Transcription failed for {filename}")
                self.fail(f"transcribe_audio returned None for {filename}")

            actual_transcription = actual_transcription.strip()
            similarity_score = self.calculate_similarity(expected_text.strip(), actual_transcription)
            # Use tqdm.write() to ensure the progress bar stays in place and print accuracy
            tqdm.write(f"{filename}: {similarity_score:.2f}% accuracy")

            total_score += similarity_score
            num_tests += 1

            self.assertGreaterEqual(similarity_score, 80, f"Low accuracy: {similarity_score:.2f}%")  # Adjust threshold if needed

        # Calculate and print average accuracy
        if num_tests > 0:
            average_accuracy = total_score / num_tests
            tqdm.write(f"\nAverage Accuracy: {average_accuracy:.2f}%")
        else:
            tqdm.write("\nNo tests were run.")

if __name__ == "__main__":
    unittest.main()